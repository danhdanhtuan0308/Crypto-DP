# Example .env file for data-pipeline-batch
# Copy to .env and fill in your values

# GCS Configuration
GCS_BUCKET=crypto-db-east1  # Single bucket for Raw/, RealTime/, and Batch/
GCS_BATCH_PREFIX=Batch  # Prefix for batch-processed parquet files
GCP_SERVICE_ACCOUNT_JSON='{"type": "service_account", "project_id": "your-project", "private_key_id": "...", "private_key": "...", "client_email": "...", "client_id": "...", "auth_uri": "...", "token_uri": "...", "auth_provider_x509_cert_url": "...", "client_x509_cert_url": "..."}'

# Kafka Configuration (for Speed Layer - data-pipeline services)
CONFLUENT_KAFKA_BOOTSTRAP_SERVERS=your-server.confluent.cloud:9092
CONFLUENT_KAFKA_API_KEY_GCS=your-api-key
CONFLUENT_KAFKA_API_KEY_SECRET_GCS=your-api-secret
KAFKA_TOPIC=BTC-USD

# Batch Processing Configuration
RAW_FLUSH_INTERVAL=60  # TESTING: 60 seconds | PRODUCTION: 3600 seconds

# Airflow Configuration
AIRFLOW_UID=50000
_AIRFLOW_WWW_USER_USERNAME=admin
_AIRFLOW_WWW_USER_PASSWORD=admin
