# Dockerfile for Airflow scheduler
FROM apache/airflow:2.8.1-python3.11

USER root
RUN apt-get update && \
    apt-get install -y --no-install-recommends build-essential curl && \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/*

USER airflow

# Copy requirements and install
COPY requirements.txt /tmp/
RUN pip install --no-cache-dir -r /tmp/requirements.txt

# Copy the ETL code (needed by DAG)
COPY raw_webhook_to_gcs.py /opt/airflow/
COPY raw_webhook_to_gcs_test.py /opt/airflow/

# Copy DAGs
COPY dags /opt/airflow/dags

# Copy startup script and make it executable
COPY --chown=airflow:root --chmod=755 start-airflow.sh /opt/airflow/

WORKDIR /opt/airflow

ENV PYTHONUNBUFFERED=1
ENV AIRFLOW_HOME=/opt/airflow
ENV AIRFLOW__CORE__EXECUTOR=SequentialExecutor
ENV AIRFLOW__CORE__LOAD_EXAMPLES=False
ENV AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=sqlite:////opt/airflow/airflow.db
ENV AIRFLOW__WEBSERVER__EXPOSE_CONFIG=True
ENV AIRFLOW__WEBSERVER__WEB_SERVER_PORT=8080
ENV AIRFLOW__WEBSERVER__WORKERS=2
ENV AIRFLOW__WEBSERVER__WEB_SERVER_WORKER_TIMEOUT=300
ENV AIRFLOW__WEBSERVER__WORKER_CLASS=sync
ENV AIRFLOW__WEBSERVER__RELOAD_ON_PLUGIN_CHANGE=False
ENV GUNICORN_CMD_ARGS="--timeout 300 --graceful-timeout 300 --keep-alive 300"

# Credentials MUST be set via environment variables in Railway (no defaults)

# Expose port 8080 for Airflow webserver
EXPOSE 8080

# Override the default entrypoint and use startup script
ENTRYPOINT []
CMD ["bash", "/opt/airflow/start-airflow.sh"]
