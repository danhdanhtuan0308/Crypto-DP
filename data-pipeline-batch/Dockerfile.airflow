# Dockerfile for Airflow scheduler
FROM apache/airflow:2.8.1-python3.11

USER root
RUN apt-get update && \
    apt-get install -y --no-install-recommends build-essential curl && \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/*

USER airflow

# Copy requirements and install
COPY requirements.txt /tmp/
RUN pip install --no-cache-dir -r /tmp/requirements.txt

# Copy the ETL code (needed by DAG)
COPY raw_webhook_to_gcs.py /opt/airflow/

# Copy DAGs
COPY dags /opt/airflow/dags

WORKDIR /opt/airflow

ENV PYTHONUNBUFFERED=1
ENV AIRFLOW_HOME=/opt/airflow
ENV AIRFLOW__CORE__EXECUTOR=SequentialExecutor
ENV AIRFLOW__CORE__LOAD_EXAMPLES=False
ENV AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=sqlite:////opt/airflow/airflow.db
ENV AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=False
ENV AIRFLOW__SCHEDULER__CATCHUP_BY_DEFAULT=False
ENV AIRFLOW__WEBSERVER__EXPOSE_CONFIG=True
ENV AIRFLOW__WEBSERVER__WEB_SERVER_PORT=8080
ENV AIRFLOW__WEBSERVER__WORKERS=2
ENV AIRFLOW__WEBSERVER__WEB_SERVER_WORKER_TIMEOUT=300
ENV AIRFLOW__WEBSERVER__WORKER_CLASS=sync
ENV AIRFLOW__WEBSERVER__RELOAD_ON_PLUGIN_CHANGE=False
ENV GUNICORN_CMD_ARGS="--timeout 300 --graceful-timeout 300 --keep-alive 300"

# Credentials MUST be set via environment variables in Railway (no defaults)

# Expose port 8080 for Airflow webserver
EXPOSE 8080

# Expose port 8080 for Airflow webserver
EXPOSE 8080

# Startup script inline
ENTRYPOINT []
CMD ["bash", "-c", "airflow db init && \
     airflow users create --username ${AIRFLOW_USERNAME:-admin} --password ${AIRFLOW_PASSWORD:-admin} \
     --firstname Admin --lastname User --role Admin --email admin@example.com || true && \
     airflow scheduler & \
     exec airflow webserver"]